data_raw <- read.csv("data/traindata_10percent.csv", stringsAsFactors = FALSE)
colnames <- read.table("data/names", skip = 1, sep = ":")
names(data_raw) <- colnames$V1
d <- dim(data_raw)
names(data_raw)[d[2]] <- "label"
sum_label <- aggregate(rep(1, d[1]),
by = list(data_raw$label),
FUN = sum)
names(sum_label) <- c("label", "count")
barplot(beside = TRUE, log10(sum_label$count),
names.arg = sum_label$label, ylim = c(0,6),
xlab = "Label", ylab = "log(Count)",
col = "Blue", main = "The distribution of labels")
library(caret)
l <- data_raw$label
sum(is.na(l))
nzvcol <- nearZeroVar(data_raw)
training <- data_raw[, -nzvcol]
training$label <- factor(training$label)
d <- dim(training)
library(randomForest)
model_rf <- train(classe ~ .,  method="rf", data=trainingPartition)
model_rf <- train(label ~ .,  method="rf", data=training)
install.packages("RWeka")
library(RWeka)
View(training)
View(training)
write.table(training,file = "cs.csv")
write.table(training,file = "cs.csv",sep=",")
weka_label_result = training[,42]
weka_label_result = training[,d[2]]
weka_training_data = training[,1:d[2]-1]
weka_tree_model = J48(as.factor(weka_label_result)~.,weka_training_data)
weka_tree_model = J48(weka_label_result~.,weka_training_data)
d
weka_training_data = training[,1:18]
weka_tree_model = J48(weka_label_result~.,weka_training_data)
weka_label_result = data_raw[,42]
weka_training_data = training[,1:41]
weka_training_data = data_raw[,1:41]
weka_tree_model = J48(weka_label_result~.,weka_training_data)
weka_tree_model = J48(as.factor(weka_label_result) ~.,weka_training_data)
weka_tree_model = J48(as.factor(weka_label_result) ~.,weka_training_data)
train_raw <- read.csv("data/traindata_10percent.csv", stringsAsFactors = FALSE)
colnames <- read.table("data/names", skip = 1, sep = ":")
names(train_raw) <- colnames$V1
d <- dim(train_raw)
names(train_raw)[d[2]] <- "label"
sum_label <- aggregate(rep(1, d[1]),
by = list(train_raw$label),
FUN = sum)
names(sum_label) <- c("label", "count")
barplot(beside = TRUE, log10(sum_label$count),
names.arg = sum_label$label, ylim = c(0,6),
xlab = "Label", ylab = "log(Count)",
col = "Blue", main = "The distribution of labels")
library(caret)
l <- train_raw$label
sum(is.na(l))
nzvcol <- nearZeroVar(train_raw)
training <- train_raw[, -nzvcol]
training$label <- factor(training$label)
d <- dim(training)
test_raw <- read.csv("data/testdata_10percent.csv", stringsAsFactors = FALSE)
# Process the data
names(test_raw) <- colnames$V1
names(test_raw)[dim(test_raw)[2]] <- "label"
colnames_train <- names(training)
testing <- test_raw[ , colnames_train]
testing$label <- as.factor(testing$label)
library(e1071)
label_result = training[,d[2]]
training_data = training[,1:(d[2]-1)]
navie_bayes_tree_model = naiveBayes(as.factor(label_result)~.,
training_data)
testing_data = testing[, 1: (d[2]-1)]
navie_bayes_pred = predict(navie_bayes_tree_model, testing_data)
golden_answer = testing[, d[2]]
navie_bayes_pred = factor(navie_bayes_pred, levels =levels(golden_answer))
NB_accuracy <- mean(golden_answer == navie_bayes_pred,na.rm = TRUE)
dim(train_raw)
dim(unique(train_raw))
dim(test_raw)
dim(unique(test_raw))
apply(train_raw, 1, paste, collapse="$$") %in% apply(test_raw, 1, paste, collapse="$$")
duplication <- apply(train_raw, 1, paste, collapse="$$") %in% apply(test_raw, 1, paste, collapse="$$")
table(duplication)
rm( list = ls() )
# Load the data
train_raw <- read.csv("data/traindata_10percent.csv", stringsAsFactors = FALSE)
# Process the data
colnames <- read.table("data/names", skip = 1, sep = ":")
names(train_raw) <- colnames$V1
d <- dim(train_raw)
names(train_raw)[d[2]] <- "label"
sum_label <- aggregate(rep(1, d[1]),
by = list(train_raw$label),
FUN = sum)
names(sum_label) <- c("label", "count")
barplot(beside = TRUE, log10(sum_label$count),
names.arg = sum_label$label, ylim = c(0,6),
xlab = "Label", ylab = "log(Count)",
col = "Blue", main = "The distribution of labels")
library(caret)
nzvcol <- nearZeroVar(train_raw)
train_raw <- train_raw[, -nzvcol]
#label into factor
training <- train_raw
training$label <- factor(training$label)
d <- dim(training)
# Load the data
test_raw <- read.csv("data/testdata_10percent.csv", stringsAsFactors = FALSE)
# Process the data
names(test_raw) <- colnames$V1
names(test_raw)[dim(test_raw)[2]] <- "label"
# Extract the same features as training data
colnames_train <- names(training)
test_raw <- test_raw[ , colnames_train]
testing <- test_raw
testing$label <- as.factor(testing$label)
library(e1071)
training_engineer <- unique(train_raw)
d <- dim(training_engineer)
numeralize <- function(col) {
# Find the classification of this column.
char <- unique(col)
# Change the type to its corresponding type.
for (i in 1: length(char)) {
col <- replace(col, col == char[i], i)
}
col
}
training_engineer <- within(training_engineer, {
protocol_type <- numeralize(protocol_type)
service <- numeralize(service)
flag <- numeralize(flag)
label <- numeralize(label)
} )
testing_engineer <- within(test_raw, {
protocol_type <- numeralize(protocol_type)
service <- numeralize(service)
flag <- numeralize(flag)
label <- numeralize(label)
} )
label_result = training_engineer[,d[2]]
training_data = training_engineer[,1:(d[2]-1)]
navie_bayes_tree_model = naiveBayes(label_result~.,
training_data)
testing_data = testing_enginee[, 1: (d[2]-1)]
testing_data = testing_engineer[, 1: (d[2]-1)]
navie_bayes_pred = predict(navie_bayes_tree_model, testing_data)
golden_answer = testing_engineer[, d[2]]
navie_bayes_pred = navie_bayes_pred, levels =levels(golden_answer)
navie_bayes_pred = factor(navie_bayes_pred, levels =levels(golden_answer))
NB_accuracy <- mean(golden_answer == navie_bayes_pred,na.rm = TRUE)
NB_accuracy
NB_accuracy <- mean(golden_answer == navie_bayes_pred,na.rm = FALSE)
NB_accuracy
rm( list = ls() )
train_raw <- read.csv("data/traindata_full.csv", stringsAsFactors = FALSE)
# Process the data
colnames <- read.table("data/names", skip = 1, sep = ":")
names(train_raw) <- colnames$V1
d <- dim(train_raw)
names(train_raw)[d[2]] <- "label"
sum_label <- aggregate(rep(1, d[1]),
by = list(train_raw$label),
FUN = sum)
names(sum_label) <- c("label", "count")
barplot(beside = TRUE, log10(sum_label$count),
names.arg = sum_label$label, ylim = c(0,6),
xlab = "Label", ylab = "log(Count)",
col = "Blue", main = "The distribution of labels")
library(caret)
nzvcol <- nearZeroVar(train_raw)
train_raw <- train_raw[, -nzvcol]
training_engineer <- unique(train_raw)
d_unique <- dim(training_engineer)
d_percent <- (d[1] - d_unique[1]) / d[1]
d_percent
d <- d_unique
numeralize <- function(col) {
# Find the classification of this column.
char <- unique(col)
# Change the type to its corresponding type.
for (i in 1: length(char)) {
col <- replace(col, col == char[i], i)
}
col
}
training_engineer <- within(training_engineer, {
protocol_type <- numeralize(protocol_type)
service <- numeralize(service)
flag <- numeralize(flag)
label <- numeralize(label)
} )
summary(training_engineer)
# Load the data
test_raw <- read.csv("data/testdata_10percent.csv", stringsAsFactors = FALSE)
# Process the data
names(test_raw) <- colnames$V1
names(test_raw)[dim(test_raw)[2]] <- "label"
# Extract the same features as training data
colnames_train <- names(training)
test_raw <- test_raw[ , colnames_train]
# Implement the numeralization feature engineering on testing dataset.
# Note: we could not delete the duplicate data in testing set.
testing_engineer <- within(test_raw, {
protocol_type <- numeralize(protocol_type)
service <- numeralize(service)
flag <- numeralize(flag)
label <- numeralize(label)
} )
# Build the model
label_result = training_engineer[,d[2]]
training_data = training_engineer[,1:(d[2]-1)]
navie_bayes_tree_model = naiveBayes(label_result~.,
training_data)
# Predict the testing
testing_data = testing_engineer[, 1: (d[2]-1)]
navie_bayes_pred = predict(navie_bayes_tree_model, testing_data)
golden_answer = testing_engineer[, d[2]]
navie_bayes_pred = factor(navie_bayes_pred, levels =levels(golden_answer))
# Get the accuracy
NB_accuracy <- mean(golden_answer == navie_bayes_pred,na.rm = FALSE)
NB_accuracy
rm( list = ls() )
# Load the data
train_raw <- read.csv("data/traindata_full.csv", stringsAsFactors = FALSE)
# Process the data
colnames <- read.table("data/names", skip = 1, sep = ":")
names(train_raw) <- colnames$V1
d <- dim(train_raw)
names(train_raw)[d[2]] <- "label"
sum_label <- aggregate(rep(1, d[1]),
by = list(train_raw$label),
FUN = sum)
names(sum_label) <- c("label", "count")
barplot(beside = TRUE, log10(sum_label$count),
names.arg = sum_label$label, ylim = c(0,6),
xlab = "Label", ylab = "log(Count)",
col = "Blue", main = "The distribution of labels")
library(caret)
# Clean up near zero variance features
nzvcol <- nearZeroVar(train_raw)
train_raw <- train_raw[, -nzvcol]
# Delete the duplicate data in the training set.
training_engineer <- unique(train_raw)
d_unique <- dim(training_engineer)
d_percent <- (d[1] - d_unique[1]) / d[1]
d <- d_unique
numeralize <- function(col) {
# Find the classification of this column.
char <- unique(col)
# Change the type to its corresponding type.
for (i in 1: length(char)) {
col <- replace(col, col == char[i], i)
}
col
}
training_engineer <- within(training_engineer, {
protocol_type <- numeralize(protocol_type)
service <- numeralize(service)
flag <- numeralize(flag)
label <- numeralize(label)
} )
# Load the data
test_raw <- read.csv("data/testdata_10percent.csv", stringsAsFactors = FALSE)
# Process the data
names(test_raw) <- colnames$V1
names(test_raw)[dim(test_raw)[2]] <- "label"
# Extract the same features as training data
colnames_train <- names(training)
test_raw <- test_raw[ , colnames_train]
# Implement the numeralization feature engineering on testing dataset.
# Note: we could not delete the duplicate data in testing set.
testing_engineer <- within(test_raw, {
protocol_type <- numeralize(protocol_type)
service <- numeralize(service)
flag <- numeralize(flag)
label <- numeralize(label)
} )
label_result = training_engineer[,d[2]]
training_data = training_engineer[,1:(d[2]-1)]
navie_bayes_tree_model = naiveBayes(label_result~.,
training_data)
# Predict the testing
testing_data = testing_engineer[, 1: (d[2]-1)]
navie_bayes_pred = predict(navie_bayes_tree_model, testing_data)
golden_answer = testing_engineer[, d[2]]
navie_bayes_pred = factor(navie_bayes_pred, levels =levels(golden_answer))
# Get the accuracy
NB_accuracy <- mean(golden_answer == navie_bayes_pred,na.rm = FALSE)
NB_accuracy
